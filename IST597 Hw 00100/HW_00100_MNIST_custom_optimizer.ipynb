{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briannewjawn/IST597/blob/ist-hw00100/HW_00100_MNIST_custom_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX6v3H_xkwno"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "import pdb; \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmNuYeJjk6ih",
        "outputId": "494b1467-cb56-4e5d-d824-425ac2c68e07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Glaatwuek7kh",
        "outputId": "7e662545-e472-4369-f798-56e22ba92af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UEZWcoVlCBr",
        "outputId": "0a26988f-c23a-4560-9ce8-4e8cb0e07abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ],
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1LJCIy4lGs7",
        "outputId": "0803f3b8-a730-4963-a0c6-edbe04cc006b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "\n",
        "np.max(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKJGUrGplIdc",
        "outputId": "60876163-37c5-4dcb-aabd-2c6fcd5a8a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnF0f-Lh24JI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mAZWCktlWw0"
      },
      "source": [
        "Custom Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGbCHGz6lMu8"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output,mt,vt,ut,device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output,self.mt,self.vt,self.ut, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output,mt,vt,ut, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "    self.Wvariables =[self.W1, self.W2, self.W3,self.W4]\n",
        "    \n",
        "    \n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    regularizerL1L2 = tf.keras.regularizers.L1L2()\n",
        "    lossL1L2 = tf.add_n([regularizerL1L2(v) for v in self.Wvariables ])\n",
        "    loss_x = cce(y_true_tf, y_pred_tf) + 0.3*lossL1L2\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train,epoch):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    #optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "    mt=self.mt\n",
        "    vt=self.vt\n",
        "    ut=self.ut\n",
        "\n",
        "    alpha = 2e-3\n",
        "    beta_1 = 0.9\n",
        "    beta_2 = 0.999\n",
        "    beta_3 = 0.999987\n",
        "    epsilon = 1e-8\n",
        "    t = epoch + 1\n",
        "\n",
        "    \n",
        "    for i in range(len(grads)):\n",
        "      #update bias moment estimate\n",
        "      mt[i] = beta_1*mt[i]+ (1-beta_1)*grads[i]\n",
        "      vt[i] = beta_2*vt[i]+(1-beta_2)*grads[i]**2\n",
        "      ut[i] = beta_3*ut[i] +(1-beta_3)*grads[i]**3\n",
        "\n",
        "      #update bias correction\n",
        "      mhat = mt[i]/(1-beta_1**t)\n",
        "      vhat = vt[i]/(1-beta_2**t)\n",
        "      uhat = ut[i]/(1-beta_3**t)\n",
        "      #update weights\n",
        "      self.variables[i].assign(self.variables[i] - alpha*mhat/(tf.sqrt(vhat) +((np.cbrt(uhat))*epsilon)))\n",
        "\n",
        " \n",
        "\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9zqHw2-lPSl",
        "outputId": "2c097f78-9e38-4670-e512-3eda56d377fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Accuracy: 0.9630\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.002125102233886719 \n",
            "\n",
            "Validation Accuracy: 0.9621\n",
            "\n",
            "Train Accuracy: 0.9764\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.000842691650390625 \n",
            "\n",
            "Validation Accuracy: 0.9692\n",
            "\n",
            "Train Accuracy: 0.9814\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0006024306869506836 \n",
            "\n",
            "Validation Accuracy: 0.9707\n",
            "\n",
            "Train Accuracy: 0.9849\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0004700415802001953 \n",
            "\n",
            "Validation Accuracy: 0.9719\n",
            "\n",
            "Train Accuracy: 0.9876\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.00037949153900146486 \n",
            "\n",
            "Validation Accuracy: 0.9734\n",
            "\n",
            "Train Accuracy: 0.9900\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.00031015497207641604 \n",
            "\n",
            "Validation Accuracy: 0.9737\n",
            "\n",
            "Train Accuracy: 0.9924\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.00025485254287719726 \n",
            "\n",
            "Validation Accuracy: 0.9747\n",
            "\n",
            "Train Accuracy: 0.9937\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.000209322452545166 \n",
            "\n",
            "Validation Accuracy: 0.9746\n",
            "\n",
            "Train Accuracy: 0.9945\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0001721315574645996 \n",
            "\n",
            "Validation Accuracy: 0.9748\n",
            "\n",
            "Train Accuracy: 0.9954\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00014041682243347167 \n",
            "\n",
            "Validation Accuracy: 0.9740\n",
            "\n",
            "Total time taken (in seconds): 201.42\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD4CAYAAAAQP7oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZJ0lEQVR4nO3df5Bd5X3f8fcHSahVMllstEOwfu2mLMmsQozdWw1uag+TjQdBTZZ0NPYyqq266mxJpdZu0qZSNRNqzWwH2iZQJ4BnGynIdKMfI7v2kuIQLGWC/whCV1hBSFhhLSFpFRltBF2SygVWfPvHfRbuXq60z0qrPXd3P6+ZO3vOc57z3OfcGfThnOc55ygiMDMzy3FN0R0wM7Ppw6FhZmbZHBpmZpbNoWFmZtkcGmZmlm1u0R24mhYuXBgtLS1Fd8PMbFo5cODAX0dEc71tMzo0WlpaKJfLRXfDzGxakXTiYtt8ecrMzLI5NMzMLJtDw8zMsjk0zMwsm0PDzMyyOTTq6DvUR8vDLVzzlWtoebiFvkN9RXfJzKwhzOgpt5ej71Af3U92c/6d8wCcGD5B95PdAKy+ZXWRXTMzK5zPNGps2rPpvcAYdf6d82zas6mgHpmZNQ6HRo2TwycnVG5mNps4NGosbVo6oXIzs9nEoVGjp6OHBfMWjClbMG8BPR09BfXIzKxxODRqrL5lNb1397KsaRlCLGtaRu/dvR4ENzMDNJPfEV4qlcIPLDQzmxhJByKiVG+bzzTMzCybQ8PMzLI5NMzMLJtDw8zMsjk0zMwsW1ZoSFop6aikAUkb6myfL2ln2r5PUkvVto2p/KikO1LZEkl/KumIpMOSvlRV/8OSnpH0Svr7oVQuSV9Nbb0o6eNXevBmZjYx44aGpDnAI8CdQDtwr6T2mmprgTci4ibgIeDBtG870AUsB1YCj6b2RoDfiIh24DZgXVWbG4A9EdEG7EnrpO9vS59u4LHLOmIzM7tsOWcaK4CBiDgWEW8DO4DOmjqdwLa0vBvokKRUviMi3oqI48AAsCIizkTECwAR8TfAy8CiOm1tA+6pKv96VDwHXCfpxgker5mZXYGc0FgEnKpaH+T9f+A/UCciRoBh4PqcfdOlrI8B+1LRDRFxJi3/CLhhAv1AUreksqTy0NDQ+EdnZmbZCh0Il/STwDeAL0fEm7Xbo3K7+oRuWY+I3ogoRUSpubl5knpqZmaQFxqngSVV64tTWd06kuYCTcC5S+0raR6VwOiLiG9W1Xlt9LJT+nt2Av0wM7OrKCc09gNtklolXUtlYLu/pk4/sCYtrwL2prOEfqArza5qpTKI/Xwa79gCvBwRv3OJttYA364q/0KaRXUbMFx1GcvMzKbAuK97jYgRSeuBp4E5wNaIOCxpM1COiH4qAfCEpAHgdSrBQqq3CzhCZcbUuoi4IOkfAZ8HDkk6mL7qP0bEU8ADwC5Ja4ETwGfT9qeAu6gMpp8HvjgJx29mZhPgp9yamdkYfsqtmZlNCoeGmZllc2iYmVk2h4aZmWVzaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVk2h4aZmWVzaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVm2rNCQtFLSUUkDkjbU2T5f0s60fZ+klqptG1P5UUl3VJVvlXRW0ks1be2UdDB9Xh19SZOkFkk/rtr2tcs9aDMzuzzjvrlP0hzgEeDTwCCwX1J/RBypqrYWeCMibpLUBTwIfE5SO5W3+C0HPgJ8V9LNEXEBeBz4PeDr1d8XEZ+r+u7fBoarNv8wIm6d+GGamdlkyDnTWAEMRMSxiHgb2AF01tTpBLal5d1AR3oPeCewIyLeiojjVF7VugIgIp6l8mrYutL+nwW2T+B4zMzsKsoJjUXAqar1wVRWt05EjFA5O7g+c9+L+STwWkS8UlXWKun7kv5M0icz2zEzs0ky7uWpAt3L2LOMM8DSiDgn6e8D35K0PCLerN5JUjfQDbB06dIp66yZ2WyQc6ZxGlhStb44ldWtI2ku0AScy9z3A1Ib/wTYOVqWLnGdS8sHgB8CN9fuGxG9EVGKiFJzc/O4B2dmZvlyQmM/0CapVdK1VAa2+2vq9ANr0vIqYG9ERCrvSrOrWoE24PmM7/xl4AcRMThaIKk5Dcoj6WdSW8cy2jIzs0ky7uWpiBiRtB54GpgDbI2Iw5I2A+WI6Ae2AE9IGqAyuN2V9j0saRdwBBgB1qWZU0jaDtwOLJQ0CNwfEVvS13bxwQHwTwGbJb0DvAvcFxEXHUg3M7PJp8oJwcxUKpWiXC4X3Q0zs2lF0oGIKNXb5jvCzcwsm0PDzMyyOTTMzCybQ8PMzLI5NMzMLJtDw8zMsjk0zMwsm0PDzMyyOTTMzCybQ8PMzLI5NMzMLJtDw8zMsjk0zMwsm0PDzMyyOTTMzCybQ8PMzLJlhYaklZKOShqQtKHO9vmSdqbt+yS1VG3bmMqPSrqjqnyrpLOSXqpp6z9JOi3pYPrcNV5bZmY2NcYNjfRe7keAO4F24F5J7TXV1gJvRMRNwEPAg2nfdiqvbl0OrAQeHX3PN/B4KqvnoYi4NX2eymjLzMymQM6ZxgpgICKORcTbwA6gs6ZOJ7AtLe8GOiQple+IiLci4jgwkNojIp6l8j7xXBdty8zMpkZOaCwCTlWtD6ayunUiYgQYBq7P3Lee9ZJeTJewPjSBfpiZ2VXUiAPhjwF/D7gVOAP89kR2ltQtqSypPDQ0dDX6Z2Y2a+WExmlgSdX64lRWt46kuUATcC5z3zEi4rWIuBAR7wL/g/cvQWW1FRG9EVGKiFJzc/M4h2ZmZhORExr7gTZJrZKupTIY3V9Tpx9Yk5ZXAXsjIlJ5V5pd1Qq0Ac9f6ssk3Vi1+qvA6OyqCbdlZmaTa+54FSJiRNJ64GlgDrA1Ig5L2gyUI6If2AI8IWmAyuB2V9r3sKRdwBFgBFgXERcAJG0HbgcWShoE7o+ILcB/kXQrEMCrwL8cry0zM5saqpwQzEylUinK5XLR3TAzm1YkHYiIUr1tjTgQbmZmDcqhYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWzaFhZmbZskJD0kpJRyUNSNpQZ/t8STvT9n2SWqq2bUzlRyXdUVW+VdJZSS/VtPVfJf1A0ouS/pek61J5i6QfSzqYPl+73IM2M7PLM25oSJoDPALcCbQD90pqr6m2FngjIm4CHgIeTPu2U3n163JgJfBoag/g8VRW6xng5yPiF4C/BDZWbfthRNyaPvflHaKZmU2WnDONFcBARByLiLeBHUBnTZ1OYFta3g10SFIq3xERb0XEcWAgtUdEPEvlfeJjRMSfRMRIWn0OWDzBYzIzs6skJzQWAaeq1gdTWd066R/8YeD6zH0v5Z8D36lab5X0fUl/JumT9XaQ1C2pLKk8NDQ0ga8yM7PxNOxAuKRNwAjQl4rOAEsj4mPArwN/KOmnaveLiN6IKEVEqbm5eeo6bGY2C+SExmlgSdX64lRWt46kuUATcC5z3w+Q9M+AzwCrIyIA0iWuc2n5APBD4OaM/puZ2STJCY39QJukVknXUhnY7q+p0w+sScurgL3pH/t+oCvNrmoF2oDnL/VlklYCvwn8SkScrypvHh1El/Qzqa1jGf03M7NJMne8ChExImk98DQwB9gaEYclbQbKEdEPbAGekDRAZXC7K+17WNIu4AiVS03rIuICgKTtwO3AQkmDwP0RsQX4PWA+8ExlLJ3n0kypTwGbJb0DvAvcFxEfGEg3M7OrR+nqz4xUKpWiXC4X3Q0zs2lF0oGIKNXb1rAD4WZm1ngcGmZmls2hYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWzaFhZmbZHBpmZpbNoWFmZtkcGmZmls2hYWZm2RwaZmaWLSs0JK2UdFTSgKQNdbbPl7Qzbd8nqaVq28ZUflTSHVXlWyWdlfRSTVsflvSMpFfS3w+lckn6amrrRUkfv9yDNjOzyzNuaKT3cj8C3Am0A/dKaq+pthZ4IyJuAh4CHkz7tlN59etyYCXw6Oh7voHHU1mtDcCeiGgD9qR10ve3pU838FjeIZqZ2WTJOdNYAQxExLGIeBvYAXTW1OkEtqXl3UCHKi/47gR2RMRbEXEcGEjtERHPUnmfeK3qtrYB91SVfz0qngOuk3RjzkGamdnkyAmNRcCpqvXBVFa3TkSMAMPA9Zn71rohIs6k5R8BN0ygH0jqllSWVB4aGhrnq8zMbCIaeiA8IgKICe7TGxGliCg1NzdfpZ6Zmc1OOaFxGlhStb44ldWtI2ku0AScy9y31mujl53S37MT6IeZmV1FOaGxH2iT1CrpWioD2/01dfqBNWl5FbA3nSX0A11pdlUrlUHs58f5vuq21gDfrir/QppFdRswXHUZy8zMpsDc8SpExIik9cDTwBxga0QclrQZKEdEP7AFeELSAJXB7a6072FJu4AjwAiwLiIuAEjaDtwOLJQ0CNwfEVuAB4BdktYCJ4DPpq48BdxFZTD9PPDFyfgBzMwsnyonBDNTqVSKcrlcdDfMzKYVSQciolRvW0MPhJuZWWNxaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVk2h4aZmWVzaDSovkN9tDzcwjVfuYaWh1voO9RXdJfMzMa/I9ymXt+hPrqf7Ob8O+cBODF8gu4nuwFYfcvqIrtmZrOczzQa0KY9m94LjFHn3znPpj2bCuqRmVmFQ6MBnRw+OaFyM7Op4tBoQEublk6o3Mxsqjg0GlBPRw8L5i0YU7Zg3gJ6OnoK6pGZWYVDowGtvmU1vXf3sqxpGUIsa1pG7929HgQ3s8L50ehmZjaGH41uZmaTIis0JK2UdFTSgKQNdbbPl7Qzbd8nqaVq28ZUflTSHeO1Kel7kg6mz19J+lYqv13ScNW237qSAzczs4kb9+Y+SXOAR4BPA4PAfkn9EXGkqtpa4I2IuElSF/Ag8DlJ7VRe/boc+AjwXUk3p33qthkRn6z67m/w/jvCAb4XEZ+53IM1M7Mrk3OmsQIYiIhjEfE2sAPorKnTCWxLy7uBDklK5Tsi4q2IOE7l/d4rctqU9FPALwHfurxDMzOzyZYTGouAU1Xrg6msbp2IGAGGgesvsW9Om/cAeyLizaqyT0j6C0nfkbS8XmcldUsqSyoPDQ1lHJ6ZmeVq5IHwe4HtVesvAMsi4qPA73KRM5CI6I2IUkSUmpubp6CbZmazR05onAaWVK0vTmV160iaCzQB5y6x7yXblLSQyiWs/z1aFhFvRsTfpuWngHmpnpmZTZGc0NgPtElqlXQtlYHt/po6/cCatLwK2BuVG0D6ga40u6oVaAOez2hzFfBHEfH/Rgsk/XQaJ0HSitT3cxM7XDMzuxLjzp6KiBFJ64GngTnA1og4LGkzUI6IfmAL8ISkAeB1KiFAqrcLOAKMAOsi4gJAvTarvrYLeKCmK6uAX5M0AvwY6IqZfGeimVkD8h3hZmY2hu8INzOzSeHQMDOzbA4NMzPL5tAwM7NsDg0zM8vm0DAzs2wODbukvkN9tDzcwjVfuYaWh1voO9RXdJfMrEDj3txns1ffoT66n+zm/DvnATgxfILuJ7sB/OpZs1nKZxp2UZv2bHovMEadf+c8m/ZsKqhHZlY0h4Zd1MnhkxMqN7OZz6FhF7W0aemEys1s5nNo2EX1dPSwYN6CMWUL5i2gp6OnoB6ZWdEcGnZRq29ZTe/dvSxrWoYQy5qW0Xt3rwfBzWYxP+XWzMzG8FNuzcxsUjg0zMwsW1ZoSFop6aikAUkb6myfL2ln2r5PUkvVto2p/KikO8ZrU9Ljko5LOpg+t6ZySfpqqv+ipI9fyYGbmdnEjRsakuYAjwB3Au3AvZLaa6qtBd6IiJuAh4AH077tVF7duhxYCTwqaU5Gm/8+Im5Nn4Op7E4q7xhvA7qBxy7ngM3M7PLlnGmsAAYi4lhEvA3sADpr6nQC29LybqBDklL5joh4KyKOAwOpvZw2a3UCX4+K54DrJN2Y0X8zM5skOaGxCDhVtT6YyurWiYgRYBi4/hL7jtdmT7oE9ZCk+RPoB5K6JZUllYeGhjIOz6YDPzjRrDE04kD4RuDngH8AfBj4DxPZOSJ6I6IUEaXm5uar0T+bYqMPTjwxfIIg3ntwooPDbOrlhMZpYEnV+uJUVreOpLlAE3DuEvtetM2IOJMuQb0F/AGVS1m5/bAZyA9ONGscOaGxH2iT1CrpWioD2/01dfqBNWl5FbA3KncN9gNdaXZVK5VB7Ocv1eboOEUaE7kHeKnqO76QZlHdBgxHxJnLOmqbVvzgRLPGMe77NCJiRNJ64GlgDrA1Ig5L2gyUI6If2AI8IWkAeJ1KCJDq7QKOACPAuoi4AFCvzfSVfZKaAQEHgftS+VPAXVQG088DX7zio7dpYWnTUk4Mn6hbbmZTy48RsYZX+zIoqDw40c/BMrs6/BgRm9b84ESzxuEzDTMzG8NnGmZmNikcGmYT4JsMbbYbd/aUmVXUDsiP3mQIeHzFZg2faZhl8k2GZg4Ns2y+ydDMoWGW7WI3E/omQ5tNHBpmmXo6elgwb8GYsgXzFtDT0VNQj8ymnkPDLJNvMjTzzX1m01LfoT427dnEyeGTLG1aSk9Hj8PLJs2lbu7zlFuzacZTf61IvjxlNs146q8VyaFhNs146q8VyaFhNs146q8VyaFhNs146q8VKSs0JK2UdFTSgKQNdbbPl7Qzbd8nqaVq28ZUflTSHeO1Kakvlb8kaaukean8dknDkg6mz29dyYGbTVeNMvXXD2+cncadcitpDvCXwKeBQSrv9743Io5U1flXwC9ExH2SuoBfjYjPSWoHtgMrgI8A3wVuTrvVbVPSXcB3Up0/BJ6NiMck3Q78u4j4TO7Becqt2dXhtynObFf6Po0VwEBEHIuIt4EdQGdNnU5gW1reDXRIUirfERFvRcRxKu/3XnGpNiPiqUiA54HFEzlYM7v6PINr9soJjUXAqar1wVRWt05EjADDwPWX2HfcNtNlqc8Df1xV/AlJfyHpO5KW1+uspG5JZUnloaGhjMMzs4nyDK7Zq5EHwh+lcmnqe2n9BWBZRHwU+F3gW/V2iojeiChFRKm5uXmKumo2uzTSDC6PrUytnNA4DSypWl+cyurWkTQXaALOXWLfS7Yp6X6gGfj10bKIeDMi/jYtPwXMk7Qwo/9mNskaZQbX6NjKieETBPHe3fEOjqsnJzT2A22SWiVdC3QB/TV1+oE1aXkVsDeNSfQDXWl2VSvQRmWc4qJtSvoXwB1UBsbfHf0CST+dxkmQtCL1/dzlHLSZXZlGmcHlsZWpN+6zpyJiRNJ64GlgDrA1Ig5L2gyUI6If2AI8IWkAeJ1KCJDq7QKOACPAuoi4AFCvzfSVXwNOAH+eMuKbEbGZShj9mqQR4MdAV8zkpy2aNbjVt6wufKaUx1amnp9ya2bTVsvDLZwYPvGB8mVNy3j1y69OaV9m0pOHr3TKrZlZQ/LYytRzaJjZtOWxlann92mY2bTmsZWxrvZlMp9pmJldoUa5b2UqLpM5NMzMrlCjjK1MxWUyh4aZ2RVqlLGVqbhM5jENM7NJ0AhjK0ubltadgjyZl8l8pmFmNkNMxWUyh4aZ2QwxFZfJfEe4mZmN4TvCzcxsUjg0zMwsm0PDzMyyOTTMzCybQ8PMzLLN6NlTkoaovNBpOlsI/HXRnWgg/j3G8u/xPv8WY13J77EsIprrbZjRoTETSCpfbOrbbOTfYyz/Hu/zbzHW1fo9fHnKzMyyOTTMzCybQ6Px9RbdgQbj32Ms/x7v828x1lX5PTymYWZm2XymYWZm2RwaZmaWzaHRoCQtkfSnko5IOizpS0X3qWiS5kj6vqQ/KrovRZN0naTdkn4g6WVJnyi6T0WS9G/TfycvSdou6e8U3aepJGmrpLOSXqoq+7CkZyS9kv5+aDK+y6HRuEaA34iIduA2YJ2k9oL7VLQvAS8X3YkG8d+BP46InwM+yiz+XSQtAv4NUIqInwfmAF3F9mrKPQ6srCnbAOyJiDZgT1q/Yg6NBhURZyLihbT8N1T+UVhUbK+KI2kx8I+B3y+6L0WT1AR8CtgCEBFvR8T/KbZXhZsL/F1Jc4EFwF8V3J8pFRHPAq/XFHcC29LyNuCeyfguh8Y0IKkF+Biwr9ieFOph4DeBd4vuSANoBYaAP0iX635f0k8U3amiRMRp4L8BJ4EzwHBE/EmxvWoIN0TEmbT8I+CGyWjUodHgJP0k8A3gyxHxZtH9KYKkzwBnI+JA0X1pEHOBjwOPRcTHgP/LJF16mI7StfpOKmH6EeAnJP3TYnvVWKJyb8Wk3F/h0GhgkuZRCYy+iPhm0f0p0C8CvyLpVWAH8EuS/mexXSrUIDAYEaNnnruphMhs9cvA8YgYioh3gG8C/7DgPjWC1yTdCJD+np2MRh0aDUqSqFyzfjkifqfo/hQpIjZGxOKIaKEywLk3Imbt/0lGxI+AU5J+NhV1AEcK7FLRTgK3SVqQ/rvpYBZPDKjSD6xJy2uAb09Gow6NxvWLwOep/F/1wfS5q+hOWcP410CfpBeBW4H/XHB/CpPOuHYDLwCHqPy7NqseKSJpO/DnwM9KGpS0FngA+LSkV6icjT0wKd/lx4iYmVkun2mYmVk2h4aZmWVzaJiZWTaHhpmZZXNomJlZNoeGmZllc2iYmVm2/w+8mYp1nEMSSQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "mt,vt,ut =[0]*8,[0]*8,[1e-6]*8  \n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output,mt,vt,ut, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(3455)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs,epoch)\n",
        "\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVzxHIDDHvEe",
        "outputId": "509c3b98-7a6d-4265-f02f-0a7a472c7b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test CCE: 0.0005\n",
            "test accuracy = 87.9000015258789\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_test_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(25, seed=epoch*(3455)).batch(128)\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total +mlp_on_cpu.loss(preds, outputs)\n",
        "  \n",
        "  \n",
        "  for i in range(preds.shape[0]):\n",
        "    if tf.argmax(preds[i])== tf.argmax(outputs[i]):\n",
        "      correct_test_prediction = correct_test_prediction + 1.0\n",
        "  \n",
        "\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "accuracytest = correct_test_prediction/X_test.shape[0]*100\n",
        "print('Test CCE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "print('test accuracy = {}'.format(accuracytest))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW 00100 MNIST custom optimizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObq06D0wako3l6dWdxRahW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
